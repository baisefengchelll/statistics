{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ibm = pd.read_csv('ibm.csv',encoding='utf-8')\n",
    "df_lx = pd.read_csv('ibm.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ibm = df_ibm['正文']\n",
    "text_lx = df_lx['正文']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今年8月，有个消息轰动了中国科技界，尤其是圈。由中国科研团队研发的“天机”芯片登上了《自然》杂志封面。相关文章展示了清华大学施路平团队研发的世界首款异构融合类脑芯片，它既可支持脉冲神经网络又支持人工神经网路，并且公布了利用“天机芯片”完成自行车自动驾驶的实验视频。这件事给投资界、产业界的直接影响，是在近段时间“类脑芯片”和“类脑计算”相关的投融资、并购与创业公司突然多了起来。“类脑”相关的会议活动也突然增加。虽然说“类脑热”还远远谈不上，但这个领域的突然升温却是真实可见的。如果我们把目光放得更远一点，类脑芯片确实在这几年有了大规模的爆发。各大学实验室以及科技巨头纷纷拿出了类脑芯片产品，也有不少专家学者认为，人工智能要经历简单人工智能、深度人工智能、通用人工智能三个阶段。而今天的深度学习代表了第二阶段的开始，类脑计算则是通用智能大门的钥匙。事已至此，可能给大众的感觉是，类脑芯片已经是注定的未来，人类已经借由它找到了通向强人工智能的门径。然而果真如此吗？类脑芯片是否就是的终极答案，今天还埋藏着太多不确定性。而想要客观认识类脑芯片的未来，我们可能必须要把时间倒回一些，先理解它的过去。一段人类认识神经与大脑的过去。神经行为学：之外的另一条路从人类的大脑和智慧中，抽取提炼某种技术，是一件源远流长的工作。能不能让机械像人类一样识别、判断和思考，最终发展出了今天的。而在另一项“兄弟研究”里，却一步步发展出了今天的类脑芯片——换言之，类脑芯片的起点某种程度上来说跟没啥关系。因为它类的是青蛙的脑。早在16世纪，达芬奇就在手稿中分析过无头青蛙也能活的现象，某种程度上来说他发现了生物电和中枢神经系统的秘密。但是我们知道达芬奇手稿近世才被披露，所以这个发现就像他很多惊天发明一样变成了“达芬奇的秘密”。1786年，伽格尼发现了青蛙挂在金属栅栏上腿会抽动的现象，继而一步步建立了早期生物电学。沿着青蛙们以高贵牺牲精神开拓的道路，人类逐渐发现了生物电和神经系统的奥秘。即生物的神经运转，是依靠生物电刺激神经元节点，最终实现了大脑控制机体的网状神经结构。由这个结构开始，神经学界很自然就会思考另一个问题：既然动物是依靠神经元来传递信息、进行控制的，那么这种控制是如何发生的呢？围绕这个问题，人类在20世纪开始漫长的，对神经传递、神经动力的研究，并在1963年完成了神经行为学的术语概念确认。这个学科中，研究者从生物、解剖、神经反射等多个角度提出了关于神经元的行为学模型。其中很多关于神经元计算的讨论，甚至早于概念的提出。我们知道，今天人工神经网络是的基石，但人工神经网络的提出，其实只是上世纪70年代，和计算机学界对神经元研究的一次借鉴，主要是模仿了神经元分层处理的特征。它的基础还是坐落在统计学和控制论的概念上。但随着和现代计算的不断发展，作为“兄弟学科”的神经行为学自身也在进步。于是就有人联想到了，能不能直接整体移植神经元系统，在现实世界里，把类似动物大脑中神经元行为的动力机制变成一种运算机制？之所以要这么干，主要还是临近21世纪，人类发现冯诺依曼架构不断抵近极限。一种从根儿上不同于经典计算的计算架构，或许是最一劳永逸的解决办法。量子计算是一种解决方式，而全仿生神经元行为学的解决方案则是另一种——这一种在大部分时候就被简称为类脑计算。事实上，类脑计算中除了神经元行为学的仿生计算，也还要其他计算方式。但今天，毫无疑问模仿神经元行为是最成功的一种，于是我们今天看到的二者大体是可以划等号的。毕竟人脑肯定是最好的计算机，加上想发展，那么类似人脑结构的计算方式显然极具魅惑。于是类脑计算在众多新计算形式中天然占据着加分项，而又过了几十年，摩尔定律的极限愈发明显的今天，类脑计算也确实拿出了一些成绩。类脑计算：比特之外的另一条路想要了解芯片化的类脑计算之前，我们还要先了解两个东西：和人造突触。上面咱们说过了，人工神经网络（），本质上还是一种基于统计学递归原理所构建的计算架构。那么想要搞类脑计算，就需要一种更仿生大脑神经元运作的计算架构。这种架构应该体现出人脑计算的高效、精准和连续性，从而对存储分离的冯诺依曼架构提出挑战。是不是有这种东西呢？还真有。这就是今天类脑芯片们的基础检验标准：脉冲神经网络。1952年，发现了神经学的功能的离子学说和突触电位的诺贝尔医学奖得主，艾伦·劳埃德·霍奇金爵士提出了脉冲神经网络这种神经行为学模型。的价值在于，它描述了神经元之间的电位是如何产生和流动的，它认为神经元之间的交换主要靠“神经递质”来产生化学放电，从而在神经网络中实现复杂和可变的神经系统交互。这一发明来到了计算世界，就变成了一种高度模仿神经元的计算架构。它用发生脉冲的仿生来模拟神经元电位，构成了一种独特的网络结构。今天，已经在很多领域，比如低功耗和通用处理能力证明了自己的优秀。但是对于很多说一定是的进化，是下一代神经网络，这个说法有失偏颇。事实上，的出现并不比晚。说白了要有用早就用了，真正让它停留在实验室中的，还是缺乏实际的任务处理能力。但就像大规模并行计算重新激活了沉睡几十年的一样，的未来谁又说得准呢？类脑计算的另一个关键点，是计算节点的问题。我们知道，比特计算的节点是晶体管的导电开关。而类脑计算则要求模拟出与人类神经元相似的计算节点，来实现非比特计算的另一条路。这也就是说，我们需要人造神经突触。今天关于如何模拟，或者制造人工突触，已经有相当多的探索。但整体而言新材料还有这样那样的问题，能够量产的类脑芯片，基本还是用电路模拟人造突触的方式来实现类脑计算。这样做对工艺要求很高，生产效率地下，其实并非长久之计。沿着这两条路，人类慢慢就摸到了类脑芯片的大门。2011年，发布了芯片，这也是人类用电路模拟神经行为学的开端。2014年更新了第二代，功耗达到了平方厘米消耗20毫瓦，印证了类脑芯片的低功耗价值，也在一些任务上印证了类脑芯片的实际工作能力。而紧随其后的，想想也知道应该是英特尔。2017年，英特尔发布了类脑芯片，其拥有13万个人造突触。今年7月，英特尔发布了号称业界首个大规模神经形态计算系统。这个系统由64块组合而成，已经可以在自动导航、陆续规划等需要高效执行的任务中带来高于的功耗和处理能力。除此之外，业界比较出名的类脑芯片还有高通的，以及一些高校实验室和创业公司发明的芯片。吃瓜群众一致表示，这个场子现在就缺谷歌了。事实上，某种程度上看2019年是类脑芯片爆发应用潜力的一年。无论是中国的天机，还是英特尔和的类脑芯片，都已经在今天被证明了在低功耗和超高速反应上，具有值得期待的效果。这可能给领域的一些相关任务，比如非监督学习、快速定位、路径规划上带来帮助。但是客观来说，类脑芯片并不是完全成熟的。虽然主流科技公司纷纷布局，中国浙大的“达尔文”芯片、清华的“天机”芯片都已经在路上。但类脑芯片距离真正确立产业价值，从实验室步入现实世界，还有很长的路要走。已知的，未知的：类脑芯片的今天类脑芯片到底是什么？是人类的朗基努斯枪，还是唐·吉歌德面向风车的宣言？或许我们真的没有必要在今天就给出答案。计算史上从来不是每一次尝试都必须成功，同时很多失败也具有伟大的价值。综合来看，今天类脑芯片的发展至今，已经可以明确它的几大优势和特性，也就是类脑芯片的光明面：1、像人脑一样的存算一体，打破了存储计算分离的架构，这是类脑计算的核心突破。2、功耗极低，并且不会因计算任务的架构复杂化而功耗激增。这终于让计算耗电和散热两大难题找到了新的方向。3、可能更适合代表的类神经元计算架构，在未来发展之路上想想无限。并且具备架构灵活，阵列化计算效率不衰减等等优点。当然，最根本的优势在于，类脑计算可以绕开比特编程和摩尔定律。在算力极限面前，是跟量子计算一样都是人类的主要救生船。但是光明面的背后当然就是阴影，也要确实看到的是，类脑芯片在今天还有极大的不确定性，尤其是有一些基础问题无从解答。比如类脑芯片的任务性处理能力差、算力水平过低。第一代甚至无法处理任何有价值的任务。虽然经过几年的发展，类脑计算可以处理的任务越来越多，但是要看到这些任务都有严苛的先决条件。对于绝大部分计算目标来说，类脑芯片都表示带不动。另一方面，用电子电路模拟人造突触，是极其不划算的一件事。它要花费极高的工艺与技术成本，来实现效率并不高的神经元模拟。所以面向未来，更多人认为一定要找到可以代替经晶体管的，属于类脑计算的新材料——但是这个材料是什么，如何才能做到像硅晶片一样便宜，今天都是未知数。另一方面，适配类脑计算的架构、算法、编程方案等等也处在广泛的空白期。总体来看，类脑芯片今天就像一片新的开发区，附近有机场，有铁路，但其他东西都还停留在开发方案上。尤其在我们身边，还要特别警惕一件事，那就是类脑芯片虚假繁荣带来的危险。火了之后，更未来更的技术成为投融资与政府扶持热点，是一件很自然的事。但类脑计算和类脑芯片，事实上还有非常远的路要走。今天在产业中讨论它，很多时候都是漫无边际的非理性畅想。笔者曾经参加过一些地方产业组织牵头举办的类脑芯片活动，现场讨论莫衷一是，产学各界代表完全没有在统一的技术逻辑上展开对话。乱拳打死老师傅模式的盲目发展类脑，很可能最终留下“遇事不决，量子力学”般的一摊浆糊。回到类脑芯片的真实发展路径，今天的类脑芯片，本质上还处在有太多不确定性的实验室探索阶段。它的进步在真实发生，中国也确实站在非常具有想象力的起跑线上，但想让类脑芯片为世界贡献些什么，我们可能还要拿出更多，更多的，以及更多的耐心。不知道大家看过电影《富春山居图》没有？那是一部划时代的烂片，但它主题曲的几句歌词，非常适合放在这里作为结尾：“反正你的亲吻无凭无证就随天机而死天意而生只要答案，不要问”\n"
     ]
    }
   ],
   "source": [
    "text_ibm = text_ibm.apply(lambda x: re.sub(r'[a-zA-Z]', '', x))\n",
    "text_lx = text_lx.apply(lambda x: re.sub(r'[a-zA-Z]', '', x))\n",
    "text_ibm = text_ibm.replace(r'\\s+', '', regex=True)\n",
    "text_lx = text_lx.replace(r'\\s+', '', regex=True)\n",
    "text_ibm = text_ibm.apply(lambda x: re.sub(r'[^\\w\\s.,!?;:()[]{}<>“”‘’——、。，．！？；：【】（）《》]', '',x))\n",
    "text_lx = text_lx.apply(lambda x: re.sub(r'[^\\w\\s.,!?;:()[]{}<>“”‘’——、。，．！？；：【】（）《》]', '',x))\n",
    "text_ibm = text_ibm.apply(lambda x: re.sub(r'640\\?_=+', '', x))\n",
    "text_ibm = text_lx.apply(lambda x: re.sub(r'640\\?_=+', '', x))\n",
    "print(text_ibm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载BERT模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', \n",
    "                       padding=True, truncation = True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ibm = text_ibm.apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lx = text_lx.apply(extract_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = text_ibm.tolist() + text_lx.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(all_features)\n",
    "\n",
    "KMeans = KMeans(n_clusters=2, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
